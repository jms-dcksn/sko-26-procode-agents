{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f3627dc",
   "metadata": {},
   "source": [
    "# Bonus Time\n",
    "\n",
    "We'll start with the agent we built in ex 1, but add a UiPath-hosted MCP server, so the agent can invoke a MCP tool.\n",
    "\n",
    "There is an MCP Server created in AMER Presales folder in staging.uipath.com/uipathlabs - the URL for this srever is: https://staging.uipath.com/uipathlabs/Playground/agenthub_/mcp/4332993a-91e0-4b3c-84ca-c4b47bf6e386/uipath-genai-websummary\n",
    "\n",
    "The MCP server contains one tool - the GenAI Activities Web Summary activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09728aee",
   "metadata": {},
   "source": [
    "## First things first\n",
    "\n",
    "Let's run some imports for libraries and set up our LLM to go through the UiPath trust layer again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b80fec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response content: {\"id\":\"chatcmpl-Cz83MjkpfOfTr8AbJYdv1Wz5OGYqu\",\"model\":\"gpt-4o-2024-11-20\",\"object\":\"chat.completion\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"message\":{\"content\":\"The Moon does not have a capital, as it is not a country or governed by any political entity. It is a natural satellite of Earth. However, humans have explored the Moon, and notable locations include the Apollo landing sites, such as Tranquility Base, where Apollo 11 landed in 1969.\",\"role\":\"assistant\"}}],\"created\":1768685000,\"usage\":{\"completion_tokens\":64,\"prompt_tokens\":15,\"total_tokens\":79,\"cache_read_input_tokens\":0}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The Moon does not have a capital, as it is not a country or governed by any political entity. It is a natural satellite of Earth. However, humans have explored the Moon, and notable locations include the Apollo landing sites, such as Tranquility Base, where Apollo 11 landed in 1969.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 15, 'total_tokens': 79, 'cache_read_input_tokens': 0}, 'model_name': 'gpt-4o-2024-11-20', 'finish_reason': 'stop', 'system_fingerprint': 'chatcmpl-Cz83MjkpfOfTr8AbJYdv1Wz5OGYqu', 'created': 1768685000}, id='lc_run--019bcdd7-561c-7281-bbce-b80dfd0337a4-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 15, 'output_tokens': 64, 'total_tokens': 79})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain.agents import create_agent\n",
    "from uipath_langchain.chat.models import UiPathChat\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from mcp import ClientSession\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "llm = UiPathChat(\n",
    "    model=\"gpt-4o-2024-11-20\",\n",
    "    temperature=0,\n",
    "    max_tokens=4000,\n",
    "    timeout=30,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "llm.invoke(\"What is the capital of the moon?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf53bb6",
   "metadata": {},
   "source": [
    "# Now we'll get right to business\n",
    "\n",
    "We'll define a simple agent like before, but this time wrap the agent with an MCP client. \n",
    "\n",
    "This follows a simple sequence:\n",
    "\n",
    "* Connect to MCP server\n",
    "* Read the available tools\n",
    "* Add those tools to the agent\n",
    "* Invoke the agent\n",
    "\n",
    "MCP allows us to decouple the tools definition from the agent or worflow code that runs the agent - this means that I can change the tools on the server without changing the code we're writing here. This separation speeds up development, since teams can independently build tool catalogs, while agent engineers can simply discover and add them to agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765fed10",
   "metadata": {},
   "source": [
    "### Connect to the MCP Server with a client\n",
    "\n",
    "The below code creates the connection so the client can discover which tools are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5a16f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='webSummary', description='Generate AI-based summaries of web search results, synthesizing insights from multiple sources with citations.', args_schema={'type': 'object', 'properties': {'query': {'type': 'string', 'title': 'Search', 'description': 'The natural language query to search the web for'}}, 'additionalProperties': False, 'required': ['query']}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x7d2be5074ae0>)]\n"
     ]
    }
   ],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            \"transport\": \"http\",\n",
    "            \"url\": \"https://staging.uipath.com/uipathlabs/Playground/agenthub_/mcp/4332993a-91e0-4b3c-84ca-c4b47bf6e386/uipath-genai-websummary\",\n",
    "            \"headers\": {  \n",
    "                \"Authorization\": \"Bearer \" + os.getenv(\"UIPATH_ACCESS_TOKEN\"),  \n",
    "            },  \n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()\n",
    "print(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a00d3",
   "metadata": {},
   "source": [
    "### Create the agent with the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae5a06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some recent developments in AI agents:\n",
      "\n",
      "1. **LiveRamp's Agentic Orchestration Launch** (October 2025): LiveRamp introduced a platform enabling autonomous AI agents to access identity resolution and audience activation tools, supporting marketing operations. This comes amid a significant rise in agentic AI job postings.\n",
      "\n",
      "2. **G2's AI Roundup**: Highlights include OpenAI's partnership with Bain & Company for B2B applications, ChatGPT's Search rollout for real-time information retrieval, and advancements in MarTech like Zeta Global's acquisition of LiveIntent for identity resolution.\n",
      "\n",
      "3. **Microsoft's Copilot Studio**: Microsoft announced the public preview of Copilot Studio, which allows users to create autonomous agents. This initiative is scaling into 2025.\n",
      "\n",
      "4. **C.H. Robinson's AI Agent Fleet Expansion**: The company expanded its fleet of AI agents, automating over 3 million tasks such as quoting and order processing.\n",
      "\n",
      "5. **Enterprise Adoption Trends**: By 2025, 45% of enterprises are expected to run production AI agents accessing critical systems, marking a 300% increase from 2023. Security concerns like prompt injection and model poisoning are driving the need for real-time monitoring and frameworks like NIST AI RMF.\n",
      "\n",
      "6. **AI Shopping Agents**: ChatGPT and Google have rolled out AI agents for research, comparison, and purchases. About 25% of young U.S. consumers are using AI for shopping, prompting retailers to optimize for generative engine optimization (GEO).\n",
      "\n",
      "These updates reflect rapid advancements in agentic AI systems for automation, marketing, and supply chains. For more details, you can explore sources like [G2's AI roundup](https://learn.g2.com/tech-signals-october-news-round-up) and [Microsoft's blog](https://blogs.microsoft.com/blog/2024/10/21/new-autonomous-agents-scale-your-team-like-never-before/).\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "agent = create_agent(llm, tools=tools)\n",
    "\n",
    "### Invoke the agent\n",
    "\n",
    "result = await agent.ainvoke({\"messages\": [HumanMessage(content=\"Tell me the latest in AI Agent news\")]})\n",
    "print(result[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866f1a1",
   "metadata": {},
   "source": [
    "# Your challenge\n",
    "\n",
    "Take this agent and add structured input/output with the graph approach that we used in the main exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50237ab",
   "metadata": {},
   "source": [
    "# Extra Bonus\n",
    "\n",
    "Coding agents are continuously improving and have more recently proven to be able to write high quality code across large codebases. \n",
    "\n",
    "When using coding agents inside Cursor, Antigravity, Claude Code etc. - the most important rule to remember is that it operates like any other agent: it has an LLM, and it uses tools to gather context before taking actions. The critical piece is **context**.\n",
    "\n",
    "LLMs are great at writing code that they have seen before (i.e. code samples that were included in their pre-training, or as part of their fine-tuning / RL). This means that newer code, such as new SDKs that UiPath is releasing is not inherently known to the LLMs that power coding agents. \n",
    "\n",
    "# Builing UiPath Coded Agents with Coding Agents\n",
    "\n",
    "As the developer, you'll likely need to take extra steps to provide the right context to coding agents in order for them to accurately and precisely code the UiPath coded agent.\n",
    "\n",
    "Fortunately, our P&E team has built some convenient context tools in the Python SDK. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606647a",
   "metadata": {},
   "source": [
    "## Start a new project\n",
    "\n",
    "1. Open a new terminal (outside of Cursor or your IDE)\n",
    "2. Make a new directory `mkdir vibe-coding-agents` and navigate into it `cd vibe-coding-agents`\n",
    "3. Initialize a project with uv `uv init . --python 3.12`\n",
    "4. Create virtual environment `uv venv && source .venv/bin/activate` (Mac/Linux) or `uv venv && .venv/Scripts/activate` (Windows)\n",
    "5. Install uipath-langchain `uv add uipath-langchain`\n",
    "6. Create a new uipath agent `uipath new my-vibe-coded-agent`\n",
    "7. Authenticate `uipath auth` with --staging if you are authenticating to staging\n",
    "8. Initialize `uipath init`\n",
    "\n",
    "## Open your IDE (Cursor, Antigravity etc.)\n",
    "\n",
    "Explore the files in the project. Take special note of Agents.md and Claude.md.\n",
    "\n",
    "Claude.md is just a pointer to Agents.md. Claude.md is a markdown file that is by default loaded as context to Claude Code - so if you use Claude code, it will jump to reference Agents.md. Similarly, Agents.md is a markdown file that OpenAI Codex uses by default. In Cursor, you can tell the agent to look at these files in the settings.\n",
    "\n",
    "The real magic is when we look at what Agents.md contains and how it references 3 more markdown files located in the .agents folder.\n",
    "\n",
    "CLI_REFERENCE.md\n",
    "REQUIRED_STRUCTURE.md\n",
    "SDK_REFERENCE.md\n",
    "\n",
    "Explore the contents of these files.\n",
    "\n",
    "Note how they provide very helpful reference info and sample code - this is amazing context to provide the coding agent when prompting. \n",
    "\n",
    "In Cursor, you can tell the agent to focus on these instructions explicitly using `@` and then selecting the files you want to include in the prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504ea21",
   "metadata": {},
   "source": [
    "## Try it out\n",
    "\n",
    "In your new project, try a prompt like the below (just illustrative to show how we explicitly reference the markdown files - feel free to change the prompt or improve)\n",
    "\n",
    "'''\n",
    "refer to @.agent/REQUIRED_STRUCTURE.md , let's update the agent in @main.py to be a UiPath coded agent that uses a math tool. Create a math tool to perform typical arithmetic operations.\n",
    "Update the GraphState structure to represent an input math problem and the GraphOutput to output the answer and an explanation as two separate keys. \n",
    "The agent should solve the problem with the tool and provide a detailed written explanation.\n",
    "Update the StateGraph accordingly.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f69336",
   "metadata": {},
   "source": [
    "Next ask the coding agent to write an input.json file\n",
    "\n",
    "'''create a sample input JSON in a new file called input.json. \n",
    "SHould be in the structure according to the Input you created.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f847c",
   "metadata": {},
   "source": [
    "## Run the agent\n",
    "\n",
    "Open a terminal in your IDE.\n",
    "\n",
    "`uipath run agent --file input.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b28660",
   "metadata": {},
   "source": [
    "## Don't expect it to work!\n",
    "\n",
    "Expect issues - if you run into errors, ask the coding agent to help you solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227833b4",
   "metadata": {},
   "source": [
    "## Example output\n",
    "\n",
    "Your agent may produce something different. Example output below\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"assets/vibes.png\" width=\"700\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
